path_to_storage: "src/experiments/results"

experiment_type: "all_layer_couples_replacement_redundancy_analysis"

version: "0"

model_id: "mistralai/Mistral-7B-v0.3"
quantization: 4bit

#model_id: "EleutherAI/gpt-neo-1.3B"
#quantization: 4bit

#tokenizer_id: "EleutherAI/gpt-neo-1.3B"
tokenizer_id: "mistralai/Mistral-7B-v0.3"

benchmark_ids:
    - "hellaswag"
#    - "truthfulqa_mc1"
#    - "arc_challenge"
evaluation_args:
    truthfulqa_mc1:
        batch_size: 32
    arc_challenge:
        batch_size: 32
    hellaswag:
        batch_size: 32

#targets:
#    - ["layer_index", "mlp", "c_fc"]
#    - ["layer_index", "mlp", "c_proj"]
#replacement_procedure: "subsequent"
#num_layers: 24

targets:
    - ["layer_index", "mlp", "gate_proj"]
    - ["layer_index", "mlp", "up_proj"]
    - ["layer_index", "mlp", "down_proj"]
num_layers: 32

figure_size: [30, 30]

verbose: 1
device: "mps"
seed: 42
